{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f366365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports for manipulating and visualising data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from sklearn import metrics, neighbors\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from math import isclose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d9f913b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212, 1)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'http://archive.ics.uci.edu/ml/machine-learning-databases/image/segmentation.data'\n",
    "df = pd.read_table(path)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca4cb02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b0f4034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the classes\n",
    "majority_class = deepcopy(df)\n",
    "minority_class = deepcopy(df)\n",
    "\n",
    "minority_class = minority_class[(minority_class.target ==1)]\n",
    "majority_class = majority_class[(majority_class.target ==0)]\n",
    "\n",
    "# Clean up index\n",
    "minority_class = minority_class.reset_index()\n",
    "del minority_class['index']\n",
    "majority_class = majority_class.reset_index()\n",
    "del majority_class['index']\n",
    "\n",
    "X_train = deepcopy(minority_class)\n",
    "X_test = deepcopy(majority_class)\n",
    "y_train = X_train.target\n",
    "y_test = X_test.target\n",
    "del X_train[\"target\"]\n",
    "del X_test[\"target\"]\n",
    "\n",
    "# Scale Data\n",
    "X_train = pd.get_dummies(X_train)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "scaler.fit(X_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Fit the model and find the counterfactual-set cf(x,p) using KNeighborsClassifier\n",
    "# the CF-Set pair instances either side of decision\n",
    "# It pairs an instance in the majority space and its counterfactually-related instance in the minority space\n",
    "knn = KNeighborsClassifier(n_neighbors=1, algorithm='auto', metric='euclidean') \n",
    "knn.fit(X_train, y_train)\n",
    "for i in range(len(X_test)):\n",
    "    instance = X_test[i]\n",
    "    indxs = knn.kneighbors(X_test, return_distance=False)\n",
    "    break\n",
    "rows = indxs[0: , :]\n",
    "dic = pd.DataFrame(rows)\n",
    "list_of_counterfactual = dic[0].to_list()\n",
    "counterfactual_instances = minority_class.iloc[list_of_counterfactual]\n",
    "factual_instances = deepcopy(majority_class)\n",
    "\n",
    "# Clean up index\n",
    "counterfactual_instances = counterfactual_instances.reset_index()\n",
    "del counterfactual_instances['index']\n",
    "majority_class = majority_class.reset_index()\n",
    "del majority_class['index']\n",
    "\n",
    "# Select the number of synthetic esmaples to be generated\n",
    "for idx, row in df.iterrows():\n",
    "    num_of_maj = df[df.target == 0].shape[0]\n",
    "    num_of_min = df[df.target == 1].shape[0]\n",
    "    num_syn = num_of_maj - num_of_min  \n",
    "    factual_instances = factual_instances.head(n=num_syn)\n",
    "    counterfactual_instances = counterfactual_instances.head(n=num_syn)\n",
    "\n",
    "# Define the Tolerance and the Feature Differences\n",
    "# Define the mean and standard deviation for each feature\n",
    "for column in df:\n",
    "    mean_values = df.apply(np.mean).tolist()\n",
    "    std_values = df.apply(np.std).tolist()\n",
    "    mean_column = pd.DataFrame(mean_values, columns=['Mean'])\n",
    "    std_column = pd.DataFrame(std_values, columns=['STD'])\n",
    "    mean_std = pd.concat([mean_column,std_column], axis=1)\n",
    "\n",
    "# Calculate the tolerance\n",
    "mean_std[\"std_value\"] = 0\n",
    "mean_std[\"tolerance\"] = 0\n",
    "for ind, row in mean_std.iterrows():\n",
    "    mean_std.loc[ind, \"std_value\"] = row['Mean'] + (0.2*row['STD'])\n",
    "for ind, row in mean_std.iterrows():\n",
    "    mean_std.loc[ind, \"tolerance\"] = row['std_value'] - (row['Mean'])\n",
    "\n",
    "# Calculate the feature differences\n",
    "atol_list = mean_std['tolerance'].values.tolist()\n",
    "def compare_isclose(factual_instances, counterfactual_instances, atol_list):\n",
    "    feature_list = []\n",
    "    for i, col in enumerate(factual_instances.columns):\n",
    "        feature_list.append(np.isclose(factual_instances[col], counterfactual_instances[col], atol=atol_list[i]))\n",
    "        feature_df = pd.DataFrame.from_records(feature_list).T\n",
    "        return feature_df\n",
    "feature_df = compare_isclose(factual_instances, counterfactual_instances, atol_list)\n",
    "feature_df = feature_df.iloc[:, :-1]\n",
    "num_of_col= len(feature_df.columns)\n",
    "\n",
    "feature_df['similar'] = feature_df.sum(axis=1)\n",
    "\n",
    "for idx, row in feature_df.iterrows():\n",
    "    try:\n",
    "        similar = feature_df.loc[idx, 'similar']\n",
    "        feature_df.at[idx, \"differences\"] = (num_of_col - similar)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "# Applying the feature differences to the dataset and select the reuqired number of differences\n",
    "for idx, row in factual_instances.iterrows():\n",
    "    try:\n",
    "        differences = feature_df.loc[idx, 'differences']\n",
    "        factual_instances.at[idx, \"differences\"] = differences\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for idx, row in counterfactual_instances.iterrows():\n",
    "    try:\n",
    "        differences = feature_df.loc[idx, 'differences']\n",
    "        counterfactual_instances.at[idx, \"differences\"] = differences\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "factual_instances = factual_instances[(factual_instances.differences <=2) ]\n",
    "counterfactual_instances = counterfactual_instances[(counterfactual_instances.differences <=2)]\n",
    "\n",
    "del factual_instances['differences']\n",
    "del counterfactual_instances['differences']\n",
    "\n",
    "#  Define unpaired instances (x') by filtering out majority instances in CF-Set usinv merge function\n",
    "all_cases = deepcopy(df)\n",
    "all_cases = all_cases[(all_cases.target ==0)]\n",
    "merged = all_cases.merge(factual_instances, how='left', indicator=True)\n",
    "unique_x_prime = merged[merged['_merge']=='left_only']\n",
    "del unique_x_prime['_merge']\n",
    "unique_x_prime.shape\n",
    "\n",
    "# For each paired instance ð’™, from the majority class, find its nearest-neighbor unpaired instance x'\n",
    "X_train = deepcopy(unique_x_prime)\n",
    "X_test = deepcopy(factual_instances)\n",
    "y_train = X_train.target\n",
    "y_test = X_test.target\n",
    "del X_train[\"target\"]\n",
    "del X_test[\"target\"]\n",
    "\n",
    "# Scale Data\n",
    "X_train = pd.get_dummies(X_train)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "scaler.fit(X_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Fit model to find x' for each x\n",
    "knn = KNeighborsClassifier(n_neighbors=1, algorithm='auto', metric='euclidean') \n",
    "knn.fit(X_train, y_train)  \n",
    "for i in range(len(X_test)):\n",
    "    instance = X_test[i]\n",
    "    indxs = knn.kneighbors(X_test, return_distance=False)\n",
    "    break\n",
    "rows = indxs[0: , :]\n",
    "dic = pd.DataFrame(rows)\n",
    "list_of_names = dic[0].to_list()\n",
    "\n",
    "x_prime = unique_x_prime.iloc[list_of_names]\n",
    "\n",
    "# Create New Data Points (1st step: factual & counterfactual)\n",
    "del factual_instances['target']\n",
    "del counterfactual_instances['target']\n",
    "\n",
    "get_tolerance = pd.DataFrame(atol_list, \n",
    "             columns=['digits'])\n",
    "get_tolerance = get_tolerance.iloc[:-1, :]\n",
    "\n",
    "# Transfer feature values from p to p' using the tolerance of 0.5  \n",
    "p_prime = counterfactual_instances.where(counterfactual_instances.sub(factual_instances).abs().gt(get_tolerance), )\n",
    "\n",
    "# Clean up index\n",
    "x_prime = x_prime.reset_index()\n",
    "del x_prime['index']\n",
    "# Clean up index\n",
    "p_prime = p_prime.reset_index()\n",
    "del p_prime['index']\n",
    "del x_prime['target']\n",
    "\n",
    "# Transfer remaining feature values from x' to p' \n",
    "p_prime[p_prime.isnull()] = x_prime\n",
    "p_prime['target'] = 1\n",
    "\n",
    "new_train = pd.concat([df, p_prime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e72fe401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    681\n",
       "1    681\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the frequency of the target feature\n",
    "new_train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e3da468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "445dd4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    681\n",
       "1     53\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/mohammed.temraz.90/Desktop/PHD/PhD Projects/4th project/new_dataset_diff_ratio/winequality-red/4-5/wine5-4.csv\")\n",
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8b4ae416",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/mohammed.temraz.90/Desktop/PHD/PhD Projects/4th project/new_dataset_diff_ratio/winequality-red/4-5/wine5-4.csv\")\n",
    "\n",
    "# Split the classes\n",
    "majority_class = deepcopy(df)\n",
    "minority_class = deepcopy(df)\n",
    "minority_class = minority_class[(minority_class.target ==1)]\n",
    "majority_class = majority_class[(majority_class.target ==0)]\n",
    "\n",
    "# Clean up index\n",
    "minority_class = minority_class.reset_index()\n",
    "del minority_class['index']\n",
    "majority_class = majority_class.reset_index()\n",
    "del majority_class['index']\n",
    "\n",
    "X_train = deepcopy(minority_class)\n",
    "X_test = deepcopy(majority_class)\n",
    "y_train = X_train.target\n",
    "y_test = X_test.target\n",
    "del X_train[\"target\"]\n",
    "del X_test[\"target\"]\n",
    "\n",
    "# Scale Data\n",
    "X_train = pd.get_dummies(X_train)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "scaler.fit(X_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Fit the model and find the counterfactual-set cf(x,p) using KNeighborsClassifier\n",
    "# the CF-Set pair instances either side of decision\n",
    "# It pairs an instance in the majority space and its counterfactually-related instance in the minority space\n",
    "knn = KNeighborsClassifier(n_neighbors=1, algorithm='auto', metric='euclidean') \n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)    \n",
    "for i in range(len(X_test)):\n",
    "    instance = X_test[i]\n",
    "    indxs = knn.kneighbors(X_test, return_distance=False)\n",
    "    break\n",
    "rows = indxs[0: , :]\n",
    "dic = pd.DataFrame(rows)\n",
    "list_of_names = dic[0].to_list()\n",
    "counterfactual_instances = minority_class.iloc[list_of_names]\n",
    "factual_instances = deepcopy(majority_class)\n",
    "\n",
    "# Clean up index\n",
    "counterfactual_instances = counterfactual_instances.reset_index()\n",
    "del counterfactual_instances['index']\n",
    "# Clean up index\n",
    "majority_class = majority_class.reset_index()\n",
    "del majority_class['index']\n",
    "\n",
    "#  Select the number of synthetic esmaples to be generated\n",
    "for idx, row in df.iterrows():\n",
    "    num_of_maj = df[df.target == 0].shape[0]\n",
    "    num_of_min = df[df.target == 1].shape[0]\n",
    "    num_syn = num_of_maj - num_of_min  \n",
    "factual_instances = factual_instances.head(n=num_syn)\n",
    "counterfactual_instances = counterfactual_instances.head(n=num_syn)\n",
    "\n",
    "# Define the Tolerance and the Feature Differences\n",
    "# Define the mean and standard deviation for each feature\n",
    "for column in df:\n",
    "    mean_values = df.apply(np.mean).tolist()\n",
    "    std_values = df.apply(np.std).tolist()\n",
    "    mean_column = pd.DataFrame(mean_values, columns=['Mean'])\n",
    "    std_column = pd.DataFrame(std_values, columns=['STD'])\n",
    "    mean_std = pd.concat([mean_column,std_column], axis=1)\n",
    "\n",
    "# Calculate the tolerance\n",
    "mean_std[\"std_value\"] = 0\n",
    "mean_std[\"tolerance\"] = 0\n",
    "for ind, row in mean_std.iterrows():\n",
    "    mean_std.loc[ind, \"std_value\"] = row['Mean'] + (1*row['STD'])\n",
    "for ind, row in mean_std.iterrows():\n",
    "    mean_std.loc[ind, \"tolerance\"] = row['std_value'] - (row['Mean'])\n",
    "    \n",
    "# Calculate the feature differences\n",
    "atol_list = mean_std['tolerance'].values.tolist()\n",
    "def compare_isclose(factual_instances, counterfactual_instances, atol_list):\n",
    "    feature_list = []\n",
    "    for i, col in enumerate(factual_instances.columns):\n",
    "        feature_list.append(np.isclose(factual_instances[col], counterfactual_instances[col], atol=atol_list[i]))\n",
    "    feature_df = pd.DataFrame.from_records(feature_list).T\n",
    "    return feature_df\n",
    "feature_df = compare_isclose(factual_instances, counterfactual_instances, atol_list)\n",
    "feature_df = feature_df.iloc[:, :-1]\n",
    "num_of_col= len(feature_df.columns)\n",
    "feature_df['similar'] = feature_df.sum(axis=1)\n",
    "\n",
    "for idx, row in feature_df.iterrows():\n",
    "    try:\n",
    "        similar = feature_df.loc[idx, 'similar']\n",
    "        feature_df.at[idx, \"differences\"] = (num_of_col - similar)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "# Applying the feature differences to the dataset and select the reuqired number of differences\n",
    "for idx, row in factual_instances.iterrows():\n",
    "    try:\n",
    "        differences = feature_df.loc[idx, 'differences']\n",
    "        factual_instances.at[idx, \"differences\"] = differences\n",
    "    except:\n",
    "        pass\n",
    "for idx, row in counterfactual_instances.iterrows():\n",
    "    try:\n",
    "        differences = feature_df.loc[idx, 'differences']\n",
    "        counterfactual_instances.at[idx, \"differences\"] = differences\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "factual_instances = factual_instances[(factual_instances.differences <=2) ]\n",
    "counterfactual_instances = counterfactual_instances[(counterfactual_instances.differences <=2)]\n",
    "del factual_instances['differences']\n",
    "del counterfactual_instances['differences']\n",
    "\n",
    "# Define unpaired instances (x') by filtering out majority instances in CF-Set usinv merge function\n",
    "all_cases = deepcopy(df)\n",
    "all_cases = all_cases[(all_cases.target ==0)]\n",
    "merged = all_cases.merge(factual_instances, how='left', indicator=True)\n",
    "unique_x_prime = merged[merged['_merge']=='left_only']\n",
    "del unique_x_prime['_merge']\n",
    "unique_x_prime.shape\n",
    "\n",
    "# For each paired instance ð’™, from the majority class, find its nearest-neighbor unpaired instance x'\n",
    "X_train = deepcopy(unique_x_prime)\n",
    "X_test = deepcopy(factual_instances)\n",
    "y_train = X_train.target\n",
    "y_test = X_test.target\n",
    "del X_train[\"target\"]\n",
    "del X_test[\"target\"]\n",
    "\n",
    "# Scale Data\n",
    "X_train = pd.get_dummies(X_train)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "scaler.fit(X_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Fit model to find x' for each x\n",
    "knn = KNeighborsClassifier(n_neighbors=1, algorithm='auto', metric='euclidean') \n",
    "knn.fit(X_train, y_train)\n",
    "for i in range(len(X_test)):\n",
    "    instance = X_test[i]\n",
    "    indxs = knn.kneighbors(X_test, return_distance=False)\n",
    "    break\n",
    "rows = indxs[0: , :]\n",
    "dic = pd.DataFrame(rows)\n",
    "list_of_names = dic[0].to_list()\n",
    "\n",
    "x_prime = unique_x_prime.iloc[list_of_names]\n",
    "\n",
    "# Create New Data Points\n",
    "del factual_instances['target']\n",
    "del counterfactual_instances['target']\n",
    "get_tolerance = pd.DataFrame(atol_list, \n",
    "             columns=['digits'])\n",
    "get_tolerance = get_tolerance.iloc[:-1, :]\n",
    "\n",
    "# Transfer feature values from p to p_prime\n",
    "p_prime = counterfactual_instances.where(counterfactual_instances.sub(factual_instances).abs().gt(get_tolerance), )\n",
    "\n",
    "# Clean up index\n",
    "x_prime = x_prime.reset_index()\n",
    "del x_prime['index']\n",
    "\n",
    "# Clean up index\n",
    "p_prime = p_prime.reset_index()\n",
    "del p_prime['index']\n",
    "del x_prime['target']\n",
    "\n",
    "# Transfer remaining feature values from x' to p' \n",
    "p_prime[p_prime.isnull()] = x_prime\n",
    "p_prime['target'] = 1\n",
    "new_train = pd.concat([df, p_prime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "93578fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    681\n",
       "1    522\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the frequency of the target feature\n",
    "new_train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_p = unique_x_prime\n",
    "p_instances = x_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda73df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
